# -*- coding: utf-8 -*-
"""Project_Amber_Robertson.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tavvKcgpO5zwn5pGN72IXbNozZnh2QJM
"""

# Dataset found at https://grouplens.org/datasets/movielens/latest/
# and https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

ls -l

# loading data from https://grouplens.org/datasets/movielens/latest/
movies = pd.read_csv("/content/movies.csv")

ratings = pd.read_csv("/content/ratings.csv")
ratings = ratings.drop(['timestamp'], axis=1)

links = pd.read_csv("/content/links.csv")
links = links.drop(['imdbId'], axis=1)

data = movies.merge(links, on="movieId")
# user_data = data.merge(ratings,on="movieId")

data.head(1)
# user_data.head(1)

# loading data from https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata
tmdb_credits = pd.read_csv("/content/tmdb_5000_credits.csv")
tmdb_credits.columns = ['tmdbId', 'title', 'cast','crew']
tmdb_movies = pd.read_csv("/content/tmdb_5000_movies.csv")
tmdb_movies.columns = ['budget', 'genres', 'homepage', 'tmdbId', 'keywords', 'original_language',
       'original_title', 'overview', 'popularity', 'production_companies',
       'production_countries', 'release_date', 'revenue', 'runtime',
       'spoken_languages', 'status', 'tagline', 'title', 'vote_average',
       'vote_count']
col_to_drop = ['budget', 'homepage', 'tagline', 'status','release_date', 'runtime','original_language', 'production_companies',
       'production_countries', 'revenue', 'original_title', 'spoken_languages']
tmdb_movies = tmdb_movies.drop(columns=col_to_drop)
tmdb_data = tmdb_movies.merge(tmdb_credits, on="tmdbId")

tmdb_data.head(1)

print(tmdb_data.shape)
print(data.shape)

# Merging both datasets to use in project
# user_data = user_data.merge(tmdb_data, on="tmdbId")
all_data = data.merge(tmdb_data, on="tmdbId")
print(all_data.shape)
# print(user_data.shape)

"""# Data exploration and prep"""

all_data.head(3)

def check_data(dataframe, head=5):
    print(" SHAPE ".center(70,'-'))
    print('Rows: {}'.format(dataframe.shape[0]))
    print('Columns: {}'.format(dataframe.shape[1]))
    print(" TYPES ".center(70,'-'))
    print(dataframe.dtypes)
    print(" MISSING VALUES ".center(70,'-'))
    print(dataframe.isnull().sum())
    print(" DUPLICATED VALUES ".center(70,'-'))
    print(dataframe.duplicated().sum())
    print(" DESCRIBE ".center(70,'-'))
    print(dataframe.describe().T)

check_data(all_data)

all_data.columns

all_data = all_data.drop(columns=(['genres_x','title_y']))
all_data.columns = (['movieId', 'title', 'tmdbId','genres',
       'keywords', 'overview', 'popularity', 'title2','vote_average',
       'vote_count', 'cast', 'crew'])
all_data.head(3)

# user_data.columns

# user_data = user_data.drop(columns=(['genres_x','title_y']))
# user_data.columns = (['movieId', 'title', 'tmdbId','userId', 'rating','genres',
#        'keywords', 'overview', 'popularity', 'title2','vote_average',
#        'vote_count', 'cast', 'crew'])

# fields = {
#     'movieId': list,
#     'title': list,
#     'tmdbId': list,
#     'rating': list,
#     'genres': list,
#     'keywords': list,
#     'overview': list,
#     'popularity': list,
#     'title2': list,
#     'vote_average': list,
#     'vote_count': list,
#     'cast': list,
#     'crew': list
# }

# #create new DataFrame by combining rows with same id values
# user_df = user_data.groupby(user_data['userId']).aggregate(fields)
# user_df

all_data.iloc[0].cast

all_data.head()

import json
#changing columns from json to string
def json_to_string(dataframe, column, key):
  dataframe[column] = dataframe[column].apply(json.loads)
  for index, values in zip(dataframe.index,dataframe[column]):
    if column == 'crew':
      list1=[item[key] for item in values if item['job'] == 'Director']
    else:
      list1=[item[key] for item in values]
    dataframe.loc[index,column] = str(list1)
  return dataframe

all_data = json_to_string(all_data, 'keywords', 'name')
all_data = json_to_string(all_data, 'genres', 'name')
all_data = json_to_string(all_data, 'cast', 'name')
all_data = json_to_string(all_data, 'crew', 'name')

all_data.head(3)

from math import nan
tmdb_data.dropna(inplace=True)

import re
# search through each title look for each character that isn't
# a space, letter or digit and will remove it like ()
def clean_title(title):
  return re.sub("[^a-zA-Z0-9 ]","", title)

def clean_data(x):
  if isinstance(x, list):
    return[str.lower(i.replace(" ", "")) for i in x]
  else:
    # Check if director exists. If not, return empty string
    if isinstance(x, str):
      return str.lower(x.replace(" ", ""))
    else:
      return ''

#create a new column and apply the clean_title funcion to it
# all_data['clean_title'] = all_data['title'].apply(clean_data)
all_data['clean_genres'] = all_data['genres'].apply(clean_data)
all_data['director'] = all_data['crew'].apply(clean_data)
all_data['clean_keywords'] = all_data['keywords'].apply(clean_data)
all_data['clean_cast'] = all_data['cast'].apply(clean_data)
all_data['clean_overview'] = all_data['overview'].apply(clean_data)
all_data['crew'] = all_data['crew'].apply(clean_data)
all_data.head(2)

#create a new column and apply the clean_title funcion to it
all_data["clean_title"] = all_data["title"].apply(clean_title)
all_data['clean_genres'] = all_data['genres'].apply(clean_title)
all_data['director'] = all_data['crew'].apply(clean_title)
all_data['clean_keywords'] = all_data['keywords'].apply(clean_title)
all_data['clean_cast'] = all_data['cast'].apply(clean_title)
all_data['clean_overview'] = all_data['overview'].apply(clean_title)
# all_data['crew'] = all_data['crew'].apply(clean_title)

all_data.head(1)

# def list_to_str(dataframe, column):
#   dataframe[column] = dataframe[column].apply(lambda x: ', '.join(map(str, x)))
#   return (dataframe[column])

all_data.iloc[0]

all_data['overview'] = all_data['overview'].apply(lambda x:x.split())

 # clean the keywords, genre and cast column to view the list
all_data['keywords'] = all_data['keywords'].str.strip('[]').str.replace(' ','').str.replace('\'','')
all_data['genres'] = all_data['genres'].str.strip('[]').str.replace(' ','').str.replace('\'','')
all_data['cast'] = all_data['cast'].str.strip('[]').str.replace(' ','').str.replace('\'','')
all_data['crew'] = all_data['crew'].str.strip('[]').str.replace(' ','').str.replace('\'','')


all_data['keywords'] = all_data['keywords'].str.split(',')
all_data['genres'] = all_data['genres'].str.split(',')
all_data['cast'] = all_data['cast'].str.split(',')
all_data['crew'] = all_data['crew'].str.split(',')


# print(all_data['overview'].head(1))
# print(all_data['keywords'].head(1))
# print(all_data['genres'].head(1))
# print(all_data['cast'].head(1))
# print(all_data['crew'].head(1))

all_data['crew'][0]

all_data['keywords'][0]

all_data['genres'][0]

all_data.head(1)

# all_data['movie_info'] = all_data['clean_overview']+all_data['clean_genres'] +all_data['clean_keywords'] +all_data['director'] +all_data['clean_cast']#.apply(lambda x: str(x))
all_data['movie_info'] = all_data['clean_genres'] +all_data['clean_keywords'] +all_data['director'] +all_data['clean_cast']

all_data.head(1)

all_data['movie_info'][0]

col_to_select = ['movieId', 'clean_title', 'title2','clean_genres', 'director', 'clean_keywords', 'clean_cast', 'clean_overview', 'movie_info','vote_average','vote_count', 'popularity']
clean_df = all_data.copy().loc[:,col_to_select]
clean_df.head(1)

"""# Filter Data and find a score for each movie based on the vote_count and vote_average
This weighted rating will be used as a metric to rate movies.

IMDB's weighted rating (wr) which is given as :

weighted Rating(WR) = ((v/(v+m))*R) + ((m/(v+m))*C)

where,

v is the number of votes for the movie (vote_count);

m is the minimum votes required to be listed in the chart
(for a movie to feature in the charts, it must have more votes than at least 90% of the movies in the list.);

R is the average rating of the movie (vote_average);

C is the mean vote across the whole report


"""

C = all_data['vote_average'].mean() # total_mean_votes
m = all_data['vote_count'].quantile(0.9) # min_votes_required; 90%
print(f'The mean vote across the whole report is {C} out of 10.')
print(f'The minimum number of votes required to be listed in the chart is {m}.')

# find the movies that qualify for the chart by having the highest scores
chart_movies = all_data.copy().loc[all_data['vote_count'] >= m]
print(f'The number of movies that qualify to be in the chart is {chart_movies.shape}.')

# define a weighted rating based  on the IMDB formula: weighted Rating(WR) =  ((v/(v+m))R) + ((m/(v+m))C)
def weighted_rating(x, m=m, C=C):
  v = x['vote_count'] # number_of_votes
  R = x['vote_average'] # average_rating
  return ((v/(v+m)*R)) + ((m/(v+m)*C)) # ((number_of_votes/(number_of_votes+min_votes_required)*average_rating))
  #+ ((min_votes_required/(min_votes_required+number_of_votes)*total_mean_votes))

# create a new feature to track the score based on the weighted rating
chart_movies['score'] = chart_movies.apply(weighted_rating, axis=1)
clean_df['score'] = clean_df.apply(weighted_rating, axis=1)

clean_df.head(1)

chart_movies.columns

chart_movies.head(1)

# sort movies based on score
chart_movies = chart_movies.sort_values('score', ascending=False)

# print the top 10 movies
chart_movies[['movieId', 'clean_title', 'movie_info', 'vote_average',
       'vote_count', 'score', 'popularity']].head(10)

pop= chart_movies.sort_values('popularity', ascending=False)
import matplotlib.pyplot as plt
plt.figure(figsize=(12,4))

plt.barh(pop['title2'].head(10),pop['popularity'].head(10), align='center',
        color='skyblue')
plt.gca().invert_yaxis()
plt.xlabel("Popularity")
plt.title("Popular Movies")

import plotly.express as px
most_pop=pop[:10]#chart_movies[0:10]
fig =px.sunburst(
    most_pop,
    path=['title2', 'director'],
    values='popularity',
    color='popularity')
fig.show()

pop= chart_movies.sort_values('score', ascending=False)
import matplotlib.pyplot as plt
plt.figure(figsize=(12,4))

plt.barh(pop['title2'].head(10),pop['score'].head(10), align='center',
        color='skyblue')
plt.gca().invert_yaxis()
plt.xlabel("Score")
plt.title("Movies with Top Score")

high_score=chart_movies[0:10]
fig =px.sunburst(
    high_score,
    path=['title2', 'director'],
    values='score',
    color='score')
fig.show()

"""# Clean Data for graphing based on frequency"""

all_data.columns

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
# plot keywords
plt.subplots(figsize=(12,10))
list4 = []
for i in all_data['keywords']:
    list4.extend(i)

# exclude certain keywords
excluded = ['\\xa0nightgown','duringcreditsstinger', 'aftercreditsstinger', 'cameoappearancebyreallifesubject','',]
new_stopwords= set(STOPWORDS).union(excluded)
#filter out excluded keywords
filtered = [keyword for keyword in list4 if keyword not in new_stopwords]

# count the occurrence of each keyword in the column and select the top 10 after excluding certain keywords
ax = pd.Series(filtered).value_counts()[:10].sort_values(ascending=True).plot.barh(width=0.9,color=sns.color_palette('hls',10))
for i, v in enumerate(pd.Series(filtered).value_counts()[:10].sort_values(ascending=True).values):
    ax.text(.8, i, v,fontsize=12,color='white',weight='bold')
plt.title('Top keywords')
plt.show()

from PIL import Image
# word cloud for keywords
# exclude certain keywords
excluded_words = ['\\xa0nightgown','duringcreditsstinger\'', 'aftercreditsstinger\'', 'cameoappearancebyreallifesubject','', 'xa0\'']

# add excluded words to the stopwords (words that aren't included)
new_stopwords= set(STOPWORDS).union(excluded_words)

text = ' '.join(all_data['keywords'].astype(str))

# remove duplicate words
word = set(text.split())

# Create and generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200, contour_width=3, contour_color='steelblue', stopwords=new_stopwords).generate(' '.join(word))

# Plot the WordCloud image
plt.figure(figsize=(12, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Keywords', fontsize=16)
plt.show()

# Generate a list of uniqe genres mentioned in the dataset
genreList = []
#iterate over rows in the genres column and add them to a list
for index, row in all_data.iterrows():
    genres = row["genres"]
    # Iterate over genres in the list and add genres that are not already in the list
    for genre in genres:
        if genre not in genreList:
            genreList.append(genre)
genreList[:10] #now we have a list with unique genres

# plotting genres to view in terms of popularity
plt.subplots(figsize=(12,10))
list5 = []

for i in all_data['genres']:
  list5.extend(i)

ax = pd.Series(list5).value_counts()[:10].sort_values(ascending=True).plot.barh(width=0.9,color=sns.color_palette('hls',10))
for i, v in enumerate(pd.Series(list5).value_counts()[:10].sort_values(ascending=True).values):
    ax.text(.8, i, v,fontsize=12,color='white',weight='bold')
plt.title('Top Genres')
plt.show()

# word cloud for genres
text = ' '.join(all_data['genres'].astype(str))

# remove duplicate words
words = set(text.split())

# Create and generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200, contour_width=3, contour_color='steelblue', stopwords='stopwords').generate(' '.join(words))

# Plot the WordCloud image
plt.figure(figsize=(12, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Genres', fontsize=16)
plt.show()

# Generate a list of uniqe cast members mentioned in the dataset
castList = []
for index, row in all_data.iterrows():
    castMembers = row['cast']

    for cast in castMembers:
        if cast not in castList:
            castList.append(cast)
castList[:10] #now we have a list with unique cast member

# plotting cast
plt.subplots(figsize=(12,10))
list6 = []
for i in all_data['cast']:
    list6.extend(i)

# exclude certain keywords
excluded = ['Jr.']
new_stopwords= set(STOPWORDS).union(excluded)
#filter out excluded keywords
filtered = [keyword for keyword in list6 if keyword not in new_stopwords]

ax = pd.Series(filtered).value_counts()[:10].sort_values(ascending=True).plot.barh(width=0.9,color=sns.color_palette('hls',10))
for i, v in enumerate(pd.Series(filtered).value_counts()[:10].sort_values(ascending=True).values):
    ax.text(.8, i, v,fontsize=12,color='white',weight='bold')
plt.title('Top Cast Members')
plt.show()

# word cloud for genres
# exclude certain keywords
excluded_word = ['J']

# add excluded words to the stopwords (words that aren't included)
new_stopwords= set(STOPWORDS).union(excluded_word)

text = ' '.join(all_data['cast'].astype(str))

# Create and generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200, contour_width=3, contour_color='steelblue', stopwords='new_stopwords').generate(text)

# Plot the WordCloud image
plt.figure(figsize=(12, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Cast', fontsize=16)
plt.show()

all_data.head(1)

# Generate a list of uniqe cast members mentioned in the dataset
directorList = []
for index, row in all_data.iterrows():
    directors = row['crew']

    for director in directors:
        if director not in directorList:
            directorList.append(director)
directorList[:10] #now we have a list with unique cast member

all_data.head(1)

# plotting Top Directors
plt.subplots(figsize=(12,10))
list7 = []
for i in all_data['crew']:
    list7.extend(i)

ax = pd.Series(list7).value_counts()[:10].sort_values(ascending=True).plot.barh(width=0.9,color=sns.color_palette('hls',10))
for i, v in enumerate(pd.Series(list7).value_counts()[:10].sort_values(ascending=True).values):
    ax.text(.8, i, v,fontsize=12,color='white',weight='bold')
plt.title('Top Directors')
plt.show()

# word cloud for directors
# exclude certain keywords
excluded_word = ['j']

# add excluded words to the stopwords (words that aren't included)
new_stopwords= set(STOPWORDS).union(excluded_word)

text = ' '.join(all_data['crew'].astype(str))

# remove duplicate words
director_words = set(text.split())

# Create and generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200, contour_width=3, contour_color='steelblue', stopwords='stopwords').generate(' '.join(director_words))

# Plot the WordCloud image
plt.figure(figsize=(12, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Directors', fontsize=16)
plt.show()



"""# Preprocessing"""

clean_df.head(1)

clean_df.columns

from sklearn.model_selection import train_test_split
target = 'score'
y = clean_df[target]
X = clean_df.drop(columns=[target], axis=1)

#select features based on data type
from sklearn.compose import make_column_selector as selector
cat_col_selector = selector(dtype_include=object)
num_col_selector = selector(dtype_exclude=object)

cat_columns = cat_col_selector(X)
num_columns = num_col_selector(X)

print(cat_columns)
print(num_columns)

# standardize features
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

scaler = StandardScaler()
oneHot = OneHotEncoder(handle_unknown="ignore")

transformer = ColumnTransformer([
    ("one_hot_encoder", oneHot, cat_columns),
    ("scaler", scaler, num_columns)],
    remainder="passthrough")

"""# Split Data into test and train"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(
    X,
    y,
    test_size=.25,
    random_state=23
)

print(X_train.shape)
print(X_test.shape)

"""# KNearest Neighbors Regressor Model"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import make_pipeline

knn = KNeighborsRegressor(n_neighbors=5)
model = make_pipeline(transformer,knn)

model.fit(X_train, y_train)

"""# Evaluate KNN Model"""

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
y_pred1 = model.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error
meanSqErr = mean_squared_error(y_test, y_pred1)
meanAbsErr = mean_absolute_error(y_test, y_pred1)
print(f"Mean squared Error: {meanSqErr}")
print(f"Mean Absolute Error: {meanAbsErr}")

"""# Visualizing KNN Model"""

# Calculate the absolute differences between actual and predicted scores
abs_diff1 = abs(y_test - y_pred1)

# Sort the test set based on absolute differences
sorted_indices1 = abs_diff1.argsort()

# Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred1, alpha=0.7, color='b', label='Actual vs. Predicted')

# list to store top 10 closest titles
top_10_score = []

# Plot the ideal line where predicted values match actual values
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='r', linewidth=2, label='Recommended')

# Annotate only the top 10 closest points
for i in sorted_indices1[:10]:
    title = X_test['title2'].iloc[i]
    top_10_score.append(title)
    #plt.annotate(title, (y_test.iloc[i], y_pred1[i]), textcoords="offset points", xytext=(0, 5), ha='center')

# Set labels and title
plt.xlabel('Actual Score')
plt.ylabel('Predicted Score')
plt.title('KNN Regressor based on Score')
plt.legend(loc='lower right')

# Set x and y axis limits for zooming in
# plt.xlim(0,40)
# plt.ylim(0,40)

# Display metrics on the plot
# plt.text(min(y_test), max(y_test), f'MSE: {meanSqErr:.2f}\nMAE: {meanAbsErr:.2f}', verticalalignment='top', horizontalalignment='left', color='green', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))

# Show the plot
plt.show()
print('Top 10 closest movies based on Score:')
for title in top_10_score:
  print(title)

"""## encoding and training based on popularity"""

from sklearn.model_selection import train_test_split
target = 'popularity'
y = clean_df[target]
X = clean_df.drop(columns=[target], axis=1)

#select features based on data type
from sklearn.compose import make_column_selector as selector
cat_col_selector = selector(dtype_include=object)
num_col_selector = selector(dtype_exclude=object)

cat_columns = cat_col_selector(X)
num_columns = num_col_selector(X)

print(cat_columns)
print(num_columns)

# standardize features
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

scaler = StandardScaler()
oneHot = OneHotEncoder(handle_unknown="ignore")

transformer = ColumnTransformer([
    ("one_hot_encoder", oneHot, cat_columns),
    ("scaler", scaler, num_columns)],
    remainder="passthrough")

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(
    X,
    y,
    test_size=.25,
    random_state=23
)

print(X_train.shape)
print(X_test.shape)

from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import make_pipeline

knn = KNeighborsRegressor(n_neighbors=5)
model = make_pipeline(transformer,knn)

model.fit(X_train, y_train)

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
y_pred = model.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error
meanSqErr = mean_squared_error(y_test, y_pred)
meanAbsErr = mean_absolute_error(y_test, y_pred)
# print(f"The accuracy score is: {accuracy_score(y_test, y_pred)}")
print(f"Mean squared Error: {meanSqErr}")
print(f"Mean Absolute Error: {meanAbsErr}")

# Calculate the absolute differences between actual and predicted scores
abs_diff = abs(y_test - y_pred)

# Sort the test set based on absolute differences
sorted_indices = abs_diff.argsort()

# Create scatter plot
plt.figure(figsize=(10, 5))
plt.scatter(y_test, y_pred, alpha=0.7, color='b', label='Actual vs. Predicted')

# list to store top 10 closest titles
top_10_pop = []

# Plot the ideal line where predicted values match actual values
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='r', linewidth=2, label='Recommended')

# Annotate only the top 10 closest points
for i in sorted_indices[:10]:
    title = X_test['title2'].iloc[i]
    top_10_pop.append(title)
    #plt.annotate(title, (y_test.iloc[i], y_pred[i]), textcoords="offset points", xytext=(0, 5), ha='center')

# Set labels and title
plt.xlabel('Actual Popularity Value')
plt.ylabel('Predicted Popularity Value')
plt.title('KNN Regressor based on popularity')
plt.legend(loc='lower right')
plt.ylim(0,300)
plt.xlim(0,300)

# Display metrics on the plot
#plt.text(min(y_test), max(y_test), f'MSE: {meanSqErr:.2f}\nMAE: {meanAbsErr:.2f}', verticalalignment='top', horizontalalignment='left', color='green', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))
# Adjust layout to prevent clipping of labels
plt.tight_layout()
# Show the plot
plt.show()
print('Top 10 closest moie titles:')
for title in top_10_pop:
  print(title)

"""# Content based filtering"""

clean_df.head(1)

# We need to convert the words in the overview so we can determine the frequency of each word
# we will use (TF-IDF) or (term instances/total instancs) to convert the words into a matrix
# Doing this will reduce the importance of words that occur frequently in the overview

#Import TfIdfVectorizer from scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer

#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'
tfidf = TfidfVectorizer(stop_words='english')

#Replace NaN with an empty string
clean_df['clean_overview'] = clean_df['clean_overview'].fillna('')

#Construct the required TF-IDF matrix by fitting and transforming the data
tfidf_matrix_overview = tfidf.fit_transform(clean_df['clean_overview'])

#Output the shape of tfidf_matrix
tfidf_matrix_overview.shape # we can see that over 19,000 different words were used to describe about 3,500 movies

"""## recommendations using overview column"""

#identify index of a movie in our dataframe by title
# clean_df = clean_df.reset_index()
indices = pd.Series(clean_df.index, index=clean_df['title2']).drop_duplicates()

# use the tfidf_matrix to compute a similarity score using cosine_similarities()
from sklearn.metrics.pairwise import linear_kernel

# Compute the cosine similarity matrix
cosine_sim1 = linear_kernel(tfidf_matrix_overview, tfidf_matrix_overview)
cosine_sim1

"""## recommendations using movie_info column which includes genres, keywords, cast, director"""

#reset dataframe and construct new indexing
# clean_df = clean_df.reset_index()
# indices = pd.Series(clean_df.index, index=clean_df['title2']).drop_duplicates()

# We need to convert the words in the movie info so we can determine the frequency of each word
# we will use (TF-IDF) or (term instances/total instancs) to convert the words into a matrix
# Doing this will reduce the importance of words that occur frequently in the overview

#Import TfIdfVectorizer from scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer

#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'
tfidf = TfidfVectorizer(stop_words='english')

#Replace NaN with an empty string
clean_df['movie_info'] = clean_df['movie_info'].fillna('')

#Construct the required TF-IDF matrix by fitting and transforming the data
tfidf_matrix_movie_info = tfidf.fit_transform(clean_df['movie_info'])

#Output the shape of tfidf_matrix
tfidf_matrix_movie_info.shape # we can see that over 56,000 different words were used to describe about 3,500 movies

# use the tfidf_matrix to compute a similarity score using cosine_similarities()
from sklearn.metrics.pairwise import linear_kernel

# Compute the cosine similarity matrix
cosine_sim2 = linear_kernel(tfidf_matrix_movie_info, tfidf_matrix_movie_info)
cosine_sim2

"""## using CountVectorizer because we do not want to set a lower weight on directors or actors that are seen more often in a movie."""

#reset dataframe and construct new indexing
# clean_df = clean_df.reset_index()
# indices = pd.Series(clean_df.index, index=clean_df['title2']).drop_duplicates()

from sklearn.feature_extraction.text import CountVectorizer
frequency = CountVectorizer(stop_words='english')
frequency_matrix = frequency.fit_transform(clean_df['movie_info'])

from sklearn.metrics.pairwise import cosine_similarity
cosine_sim3 = cosine_similarity(frequency_matrix, frequency_matrix)

# create a dictionary to hold the different cosine_sim scores
cosine_sims = {'Overview(tfidf)': cosine_sim1, 'Movie_info(tfidf)': cosine_sim2, 'Movie_info(countVect)': cosine_sim3}

# Define a recommendation function that takes in a movie title and ouputs most similar movies
def get_recommendation(title, selected_matrix=None):
  # Use a default value of 0 if selected_matrix is not provided
  selected_matrix = 0 if selected_matrix is None else selected_matrix

  # Get the selected cosine similarity matrix
  cosine_sim = cosine_sims[selected_matrix]

  # get the index of the movie
  idx = indices[title]

  # get cosine_sim score
  sim_score = list(enumerate(cosine_sim[idx]))

  # sort the movies based on similarity score
  sim_score = sorted(sim_score, key=lambda x: x[1], reverse=True)
  sim_score = sim_score[1:11]

  #get the movie indices
  movie_indices = [i[0] for i in sim_score]

  # return teh top 10 most similar movies
  return clean_df['title2'].iloc[movie_indices]

import ipywidgets as widgets
from IPython.display import display, clear_output
#create a widget
movie_input = widgets.Text(
  value='',
  description='Movie Title:',
  disabled=False
)
matrix_dropdown = widgets.Dropdown(options=list(cosine_sims.keys()), value=list(cosine_sims.keys())[0], description='Similarity Matrix:', disabled=False)

movie_list = widgets.Output()

def on_type(data): #called whenever something is typed in textbox
  with movie_list:
    clear_output(wait=True)
    #movie_list.clear_output(wait=True)
    title = data['new']
    selected_matrix = matrix_dropdown.value
    if len(title) > 3:
      display(get_recommendation(title, selected_matrix))

movie_input.observe(on_type, names='value') # whenever something is typed in box, call function
matrix_dropdown.observe(on_type, names='value')

display(movie_input, matrix_dropdown, movie_list)

print(clean_df['movie_info'][0])

"""## Collaborative filtering
user based filtering: recommend products to a user that similar users have liked.

Item based collaborative filtering: recommends items based on thir similarity with the items that the target user rated.
"""

user_data = ratings.merge(clean_df, on='movieId')
user_data.head(2)

#pip install scikit-surprise

"""# Using SVD for Collaborative Filtering"""

# The SVD class typically represents the Singular Value Decomposition
# algorithm, which is a popular matrix factorization technique used in
# collaborative filtering.
from surprise import Reader, SVD, Dataset
from surprise.model_selection import cross_validate, train_test_split
from surprise.accuracy import rmse, mae
reader = Reader()
svd = SVD()

# load data from pandas dataframe into the surprise reader
user_df = Dataset.load_from_df(user_data[['userId', 'movieId', 'rating']], reader)

# Perform cross-validation
cross_validate(svd, user_df, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# Split the data into training and testing sets
trainset, testset = train_test_split(
    user_df,
    test_size=0.25,
    random_state=59)
# fit the model
svd.fit(trainset)

prediction1 = svd.test(testset)
print('RMSE:', rmse(prediction1))
print('MAE:', mae(prediction1))

user_df

user_data[user_data['userId'] == 1]

p = svd.predict(1,50, 3)
print(p)

user_data.head(1)

# Function to get top N movie recommendations for a specific user
def get_top_n_recommendations(model, user_id, n=10):
    # Get a list of all movie IDs
    all_movie_ids = user_data['movieId'].unique()

    # Create a DataFrame with all possible user-item pairs
    all_user_item_pairs = pd.DataFrame([(user_id, movie_id) for movie_id in all_movie_ids], columns=['userId', 'movieId'])

    # Make predictions for all user-item pairs
    all_user_item_pairs['est'] = all_user_item_pairs.apply(lambda row: model.predict(row['userId'], row['movieId']).est, axis=1)

    # Sort predictions by estimated rating in descending order
    top_n_recommendations = all_user_item_pairs.sort_values(by='est', ascending=False).head(n)

    return top_n_recommendations[['movieId', 'est']]

# Get top 10 movie recommendations
user_input = 500
top_10_recommendations = get_top_n_recommendations(svd, user_input, 10)

# Print recommendations
print(f'Recommendations for userId: {user_input}')
for idx, row in top_10_recommendations.iterrows():
    movie_id = int(row['movieId'])
    movie_title = user_data[user_data['movieId'] == movie_id]['title2'].values
    if len(movie_title) > 0:
        print(f'Movie Title: {movie_title[0]}, Estimated Rating: {row["est"]:.2}')
    else:
        print(f'Movie with ID {movie_id} not found in the dataset.')

